# ğŸš€ Kubeflow Spark Operator - Tutorial Completo  

Este repositÃ³rio contÃ©m um tutorial atualizado sobre o **Kubeflow Spark Operator**, abordando desde conceitos fundamentais atÃ© um *hands-on* completo.  

## ğŸ¯ MotivaÃ§Ã£o  

O **Spark Operator** foi recentemente migrado para a comunidade do **Kubeflow**, e algumas mudanÃ§as ocorreram no projeto. Este tutorial tem como objetivo trazer conteÃºdos atualizados sobre essa nova fase do operador, permitindo que vocÃª aproveite ao mÃ¡ximo essa integraÃ§Ã£o com o Kubernetes.  

## ğŸ“Œ O que serÃ¡ abordado?  

Este tutorial serÃ¡ dividido nos seguintes tÃ³picos:  

1. **Revisar a arquitetura do Spark**  
2. **IntroduÃ§Ã£o ao Kubeflow Spark Operator**  
3. **Hands-on de um ambiente local**  
4. **Criar uma imagem do Spark customizada com Iceberg**  
5. **Deploy de um EKS com autoscaling na AWS (Terraform + Karpenter)**  
6. **InstalaÃ§Ã£o e utilizaÃ§Ã£o do ArgoCD para deploy do Spark Operator no EKS**  
7. **Executar um job PySpark com Iceberg integrado ao S3**  
8. **Boas prÃ¡ticas de reduÃ§Ã£o de custo**  
    - InstÃ¢ncias *spot* e *on-demand*  
    - *Node-affinity* e *pod-affinity*  

---

## ğŸ“Œ Arquitetura do Apache Spark  

Vamos recapitular brevemente a arquitetura do Apache Spark em alto nÃ­vel.  

![Arquitetura do Apache Spark](<caminho_para_a_imagem>)  

No Spark, temos trÃªs componentes principais:  

- **Driver**: Gerencia a execuÃ§Ã£o do aplicativo Spark, comunicando com o *Cluster Manager*, requisitando CPU e MemÃ³ria, distribui, monitora e agenda tarefas nos *Executors*.
- **Executors**: SÃ£o processos que executam tarefas do aplicativo Spark. Eles sÃ£o responsÃ¡veis por executar o cÃ³digo do usuÃ¡rio, armazenar dados em memÃ³ria ou disco e retornar resultados ao *Driver*. SÃ£o alocados nos *Workers*.
- **Cluster Manager**: Coordena a alocaÃ§Ã£o de recursos, como CPU e memÃ³ria em um cluster. Atualmente, o Spark suporta *Standalone*, *Mesos*, *YARN* e *Kubernetes*.

---

## ğŸš€ Vantagens do Spark no Kubernetes  

O **Kubernetes** Ã© um orquestrador de containers amplamente utilizado, e sua adoÃ§Ã£o no ecossistema de Big Data vem crescendo devido Ã s suas vantagens:  

1. **Escalabilidade e gerenciamento automatizado**  
   - O Kubernetes trabalha de maneira declarativa, garantindo que os recursos necessÃ¡rios sejam provisionados automaticamente.  

2. **Ambiente unificado**  
   - Permite que times de software e dados compartilhem a mesma infraestrutura.  

3. **CustomizaÃ§Ã£o avanÃ§ada**  
   - Suporte para diferentes tipos de mÃ¡quinas, *node pools*, *affinity rules*, entre outras configuraÃ§Ãµes.  

4. **AgnÃ³stico a provedores de nuvem**  
   - Funciona tanto em ambientes gerenciados (*EKS*, *GKE*, *AKS*) quanto em clusters *on-premise*.  

5. **IntegraÃ§Ã£o com ferramentas do ecossistema Kubernetes**  
   - Observabilidade, monitoramento, *service mesh*, CI/CD, entre outras.  

---

## ğŸ“Œ IntroduÃ§Ã£o ao Kubeflow Spark Operator  

Agora que revisamos a arquitetura do Spark, vamos entender como funciona o **Kubeflow Spark Operator**.  

ğŸ“Œ **Importante**: Do ponto de vista do cÃ³digo Spark (PySpark, Spark SQL, Streaming, etc.), nada muda. A diferenÃ§a estÃ¡ na forma como declaramos e submetemos os *jobs* no Kubernetes.  

O Kubernetes trabalha com arquivos YAML declarativos, e o **Spark Operator** segue essa mesma abordagem. Para rodar um job Spark, precisamos de um manifesto YAML que descreve a execuÃ§Ã£o.  

```yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: default
spec:
  type: Python
  mode: cluster
  image: "tiagotxm/spark:3.5.3-demo-yt"
  imagePullPolicy: Always
  mainApplicationFile: "local:///opt/spark/examples/src/main/python/pi.py"
  sparkVersion: "3.5.3"
  restartPolicy:
    type: Never
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    labels:
      version: 3.5.3
    serviceAccount: spark-operator-spark
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    labels:
      version: 3.5.3

```

## ğŸ›  Hands-on: Ambiente Local

### ğŸ”¹ Passo 1: Instalar o Kind
- https://kind.sigs.k8s.io/


### ğŸ”¹ Passo 2: Criar um Cluster Kubernetes
```
kind create cluster --config infra/local/kind.yaml
```

### ğŸ”¹ Passo 3: Instalar o Spark Operator
```
helm repo add spark-operator https://kubeflow.github.io/spark-operator
helm repo update

# Install the operator into the spark-operator namespace and wait for deployments to be ready
helm install spark-operator spark-operator/spark-operator \
    --namespace spark-operator --create-namespace --wait
```

### ğŸ”¹ Passo 4: Criar um job Spark
```yaml
apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: spark-pi
  namespace: default
spec:
  type: Python
  mode: cluster
  image: "tiagotxm/spark:3.5.3-demo-yt"
  imagePullPolicy: Always
  mainApplicationFile: "local:///opt/spark/examples/src/main/python/pi.py"
  sparkVersion: "3.5.3"
  restartPolicy:
    type: Never
  volumes:
    - name: "test-volume"
      hostPath:
        path: "/tmp"
        type: Directory
  driver:
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    labels:
      version: 3.5.3
    serviceAccount: spark-operator-spark
    volumeMounts:
      - name: "test-volume"
        mountPath: "/tmp"
  executor:
    cores: 1
    instances: 1
    memory: "512m"
    labels:
      version: 3.5.3
    volumeMounts:
      - name: "test-volume"
        mountPath: "/tmp"

```

* Aplique o YAML no cluster:
```
kubectl apply -f spark-pi.yaml
```
* Para verificar o status do job:
```
kubectl get sparkapplications
```

* Para ver os logs
```
kubectl logs -f <nome-do-pod>
```

* Para deletar um job
```
kubectl delete sparkapplication <spark-app-name>
```

## ğŸ›  Criando uma Imagem do Spark Customizada

### ğŸ”¹ Criando um Dockerfile

- Para criar uma imagem base do Spark, utilize este
[Dockerfile](Dockerfile.base)

- Para instalar dependÃªncias do Iceberg, utilize este [Dockerfile.iceberg](Dockerfile)



### ğŸ”¹ Build e Push da imagem para o Docker Hub
O build serÃ¡ baseado na arquitetura do seu processador.
No meu caso, estou utilizando um processador ARM64, entÃ£o a imagem serÃ¡ construÃ­da para essa arquitetura.
```
docker build -t <seu-usuario>/spark-iceberg:latest .
docker push <seu-usuario>/spark-iceberg:latest
```

### Tip: 
Realize o build da imagem do Spark para multiarquitetura, assim vocÃª poderÃ¡ utilizÃ¡-la em diferentes processadores.
```
docker buildx build --platform linux/amd64,linux/arm64 -t <nome_da_imagem> --push .
```

---

## ğŸ“Œ Kubeflow Spark Operator em ProduÃ§Ã£o

Nesta seÃ§Ã£o, vamos abordar a execuÃ§Ã£o de um job PySpark com Iceberg integrado ao S3 em um ambiente de produÃ§Ã£o.
Para isso iremos utilizar o EKS (Elastic Kubernetes Service) da AWS configurado com autoscaling(Karpenter) e o ArgoCD para deploy do Spark Operator.

## Requisitos
- AWS CLI instalado e configurado
- K9s

### ğŸ”¹ Deploy do EKS com Karpenter
````
$ terraform init
$ terraform apply -auto-approve
````

### ğŸ”¹ AutenticaÃ§Ã£o local com cluster EKS
```
$ aws eks update-kubeconfig --name eks-data --region us-east-1                                  
```

### ğŸ”¹ Iniciando a sessÃ£o do k9s
```
$ k9s
```


## ArgoCD
Cria namespace para o Argo
````
$ kubectl create namespace argocd
````

Instala o ArgoCD
````
$ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

````
Recupera a senha do usuÃ¡rio admin para logar na UI
````
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
````

CriaÃ§Ã£o da chave SSH para autenticaÃ§Ã£o com o Argo
````
$ ssh-keygen -t rsa -b 4096 -C "<github_email>"
````


### Links Ãºteis e ReferÃªncias
- K9s - https://k9scli.io
- Karpenter - https://karpenter.sh/docs/
- IRSA - https://docs.aws.amazon.com/eks/latest/userguide/associate-service-account-role.html


